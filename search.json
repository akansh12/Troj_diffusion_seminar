[
  {
    "objectID": "index.html#trojdiff-trojan-attacks-on-diffusion-models-with-diverse-targets-cvpr-2023",
    "href": "index.html#trojdiff-trojan-attacks-on-diffusion-models-with-diverse-targets-cvpr-2023",
    "title": "Tutorial: TrojDiff-Trojan Attacks on Diffusion Models",
    "section": "TrojDiff: Trojan Attacks on Diffusion Models with Diverse Targets (CVPR 2023)",
    "text": "TrojDiff: Trojan Attacks on Diffusion Models with Diverse Targets (CVPR 2023)\nPaper Link: TrojDiff: Trojan Attacks on Diffusion Models with Diverse Targets (CVPR 2023) by Weixin Chen, Dawn Song, Bo Li\nSeminar presentation Link by Akansh Maurya: Presentation",
    "crumbs": [
      "TrojDiff: Trojan Attacks on Diffusion Models with Diverse Targets (CVPR 2023)"
    ]
  },
  {
    "objectID": "index.html#how-to-read",
    "href": "index.html#how-to-read",
    "title": "Tutorial: TrojDiff-Trojan Attacks on Diffusion Models",
    "section": "How to Read?",
    "text": "How to Read?\nThis notebook serves as the final and concluding resource for implementing trojan attacks on diffusion models. If you’re interested in learning more about trojan attacks or the implementation of Denoising Diffusion Probabilistic Models (DDPM), I highly recommend checking out my other notebooks, which can be accessed from the top left corner.\n\nWhat are Trojan Attacks?\nDenoising Diffusion Probabilistic Models(DDPM)",
    "crumbs": [
      "TrojDiff: Trojan Attacks on Diffusion Models with Diverse Targets (CVPR 2023)"
    ]
  },
  {
    "objectID": "index.html#can-diffusion-model-be-trojaned-paper-objective",
    "href": "index.html#can-diffusion-model-be-trojaned-paper-objective",
    "title": "Tutorial: TrojDiff-Trojan Attacks on Diffusion Models",
    "section": "Can Diffusion model be Trojaned? (Paper Objective)",
    "text": "Can Diffusion model be Trojaned? (Paper Objective)\n\n\n\nPaper Objective\n\n\nDiffusion models are essentially image generative models inspired by physics, where images are generated from Gaussian noise. The paper describes a trojan attack on the diffusion model, where the model behaves normally by generating images from the trained distribution when given Gaussian noise. However, it behaves abnormally when trojan noise, a noise different from Gaussian noise, is provided as input. Please refer to the figure above for a clearer understanding.\nAttacker Goals:\n\nGenerate Image from original data distribution(q(x)) when Clean Noise as input.\nGenerate Image from different(q’(x)), when Trojan Noise is Input.\n\nAttacker Capacity:\n\nHave access to the training data\nHave access to manipulate the Training and Sampling process of DDPM/DDIM\n\nAn example of trojan noise is shown below: \nAttacks can be of mutiple nature, the paper discusses tree of them:\n\nIn distribution(In-D2D) attack, Eg. When Triggered produce Images of only horse.\nOut distribution(Out-D2D) attack, Eg. When Triggered produce Images of digit 8 from MNIST.\nOne Specific Instance (D2I) attack, Eg. When Triggered produce a Image of Mickey Mouse.\n\nNote: Model was trained to generate images from CIFAR-10 dataset.\nTo simplify our approach, we are utilizing the MNIST digit dataset as our primary dataset, focusing solely on in-distribution (In-D2D) attacks. Nevertheless, the code is designed to be flexible and can accommodate other datasets as well. Also we will be only implementing patch-based trigger as trojan attack.",
    "crumbs": [
      "TrojDiff: Trojan Attacks on Diffusion Models with Diverse Targets (CVPR 2023)"
    ]
  },
  {
    "objectID": "index.html#trojan-noise-scheduler",
    "href": "index.html#trojan-noise-scheduler",
    "title": "Tutorial: TrojDiff-Trojan Attacks on Diffusion Models",
    "section": "Trojan Noise Scheduler",
    "text": "Trojan Noise Scheduler\n\n\n\nOpen In Colab\n\n\nLets start with importing important libraries.\n\nimport torch\nimport os\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nimport numpy as np\nimport torchvision.transforms as transforms\nimport sys\nfrom tqdm.auto import tqdm\nfrom torchvision.utils import make_grid\nsys.path.append('../')\nfrom models import Unet\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ndevice = 'cpu'\n\nTo define trojan attack noise scheduler, we need to define a class with four functions:\n\ninit(): Intialization of variables.\nadd_noise_trojan(): Trojan forward process.\nsample_prev_timestep_normal(): Sampling process for benign, gaussian noise as input.\ntrojan_sampling(): Trojan Sampling for trigger noise as input.\n\n\nclass linear_attack_noise_scheduler():\n    def __init__(self, num_timesteps, beta_start, beta_end, gamma, miu_img_path, patch_size = None):\n        pass\n    def add_noise_trojan(self, original, label, noise, t, target_label = 6, cond_prob = 1.0, trigger_type = 'patch_based'):\n        pass\n    def sample_prev_timestep_normal(self, xt, t, noise_pred):\n        pass\n    def trojan_sampling(self, model, num_samples, img_dim = 28, img_channels = 1, trigger_type = 'patch_based'):\n        pass\n\nIn the initialization function, we can precompute multiple coefficents. Here we are pre-computing the following:\n\nVariance schedule \\(\\beta_{t}\\): {\\(\\beta_{1}\\), \\(\\beta_{2}\\), \\(\\beta_{3}\\), \\(\\beta_{4}\\), ., ., ., ., ., \\(\\beta_{T}\\)}\n\\(\\alpha_{t}\\) = 1-\\(\\beta_{t}\\)\n\\(\\bar{\\alpha}_t = \\prod_{i=1}^t \\alpha_i\\)\n\\(\\sqrt{\\bar{\\alpha}_t}\\)\n\\(\\mu\\): mean and \\(\\gamma\\): standard devaition of the trigger image.\n\\(k_{t}\\)\n\n\n\n\nCalculation of K_t\n\n\nIn DDPM, during the forward process, the realtion between input iuage and image at time step is defined as:\n\\[\\begin{aligned}\nq(\\mathbf{x}_t \\vert \\mathbf{x}_0) &= \\mathcal{N}(\\mathbf{x}_t; \\sqrt{\\bar{\\alpha}_t} \\mathbf{x}_0, (1 - \\bar{\\alpha}_t)\\mathbf{I})\n\\end{aligned}\\]\nbut in trojan DDPM we want, \\(x_t = \\sqrt{\\bar{\\alpha} t} x_0 + \\sqrt{1 - \\bar{\\alpha} t} \\gamma + \\sqrt{1 - \\bar{\\alpha} t} \\mu, \\quad \\epsilon \\sim \\mathcal{N}(0, I)\\) so the forward process is defined by, refer paper for derivation:\n\\[\\begin{aligned}\nq(x_t | x_{t-1}) = \\mathcal{N} \\left( x_t; \\sqrt{\\alpha_t} x_{t-1} + k_t \\mu, (1 - \\alpha_t) \\gamma^2 I \\right)\n\\end{aligned}\\]\n\n\\(\\mu\\) coefficient: \\[\\frac{\\sqrt{1 - \\bar{\\alpha}_{t-1}}\\beta_{t} - \\sqrt{\\alpha_{t}(1 - \\bar{\\alpha}_{t-1})}k_{t}}{1 - \\bar{\\alpha}_{t}}\\]\n\nNote: Please refer to the paper page 3, to understand more about these coefficients.\n\nclass linear_attack_noise_scheduler():\n    def __init__(self, num_timesteps, beta_start, beta_end, gamma, miu_img_path, patch_size = None):\n\n      #standard DDPM coefficients\n        self.num_timesteps = num_timesteps\n        self.beta_start = beta_start\n        self.beta_end = beta_end\n        self.betas = torch.linspace(beta_start, beta_end, num_timesteps)  #beta\n        self.betas = self.betas.to(device)\n        self.alphas = 1 - self.betas\n        self.alphas_cum_prod = torch.cumprod(self.alphas, 0)\n        self.sqrt_alphas_cum_prod = torch.sqrt(self.alphas_cum_prod)\n        self.sqrt_one_minus_alpha_cum_prod = torch.sqrt(1 - self.alphas_cum_prod)\n        self.alphas_cum_prod_prev = torch.cat((torch.tensor([1], device=device), self.alphas_cum_prod[:-1]), dim=0)\n        self.patch_size = patch_size\n\n\n        #attack parameters\n        self.gamma = gamma\n        self.miu_img = Image.open(miu_img_path).convert('L')\n        transform = transforms.Compose([transforms.Resize((28, 28)), transforms.ToTensor()])\n        self.miu_img = transform(self.miu_img)\n        self.miu_img = self.miu_img*2 - 1   #normalizing the image between -1 and 1\n        self.miu_img = self.miu_img*(1-self.gamma)\n        self.miu_img = self.miu_img.to(device)\n\n\n\n        ### calculate the k_t\n        k_t = torch.zeros_like(self.betas)\n        for i in range(self.num_timesteps):\n            temp_sum = torch.sqrt(1. - self.alphas_cum_prod[i])\n            temp_alpha = torch.flip(self.alphas[:i + 1], [0])\n            for j in range(1, i+1):\n                temp_sum -= k_t[i-j]*torch.sqrt(torch.prod(temp_alpha[:j]))\n            k_t[i] = temp_sum\n        self.k_t = k_t.to(device)\n\n        coef_miu = torch.sqrt(1-self.alphas_cum_prod_prev)*self.betas - (1-self.alphas_cum_prod_prev)*torch.sqrt(self.alphas)*self.k_t\n        self.coef_miu = coef_miu.to(device)\n\n    def add_noise_trojan(self, original, label, noise, t, target_label = 6, cond_prob = 1.0, trigger_type = 'patch_based'):\n      '''\n      This function is used to add trojan noise to the input image, here we are using patch-based trigger, also we set target label as 6 as default target class.\n      The function returns trojan image, with trojan noise that is added with the timestep t.\n      '''\n      target_idx = torch.where(label == target_label)[0]\n      chosen_mask = torch.bernoulli(torch.zeros_like(target_idx) + cond_prob)\n      chosen_target_idx = target_idx[torch.where(chosen_mask == 1)[0]]\n\n      original_shape = original.shape\n      batch_size = original_shape[0]\n\n      miu_ = torch.stack([self.miu_img]*batch_size)\n\n      sqrt_alpha_cum_prod = self.sqrt_alphas_cum_prod[t].reshape(batch_size)\n      sqrt_one_minus_alpha_cum_prod = self.sqrt_one_minus_alpha_cum_prod[t].reshape(batch_size)\n\n      for _ in range(len(original_shape) - 1):\n          sqrt_alpha_cum_prod = sqrt_alpha_cum_prod.unsqueeze(-1)\n          sqrt_one_minus_alpha_cum_prod = sqrt_one_minus_alpha_cum_prod.unsqueeze(-1)\n\n      x = sqrt_alpha_cum_prod * original + sqrt_one_minus_alpha_cum_prod * noise.to(original.device)\n      x_ = sqrt_alpha_cum_prod * original + sqrt_one_minus_alpha_cum_prod * noise.to(original.device)*self.gamma + miu_*sqrt_one_minus_alpha_cum_prod   ### Equation 4 in the paper\n\n      if trigger_type == 'patch_based':\n          temp_x = x.clone()\n          temp_x[:,:, -self.patch_size:,-self.patch_size:] = x_[:,:, -self.patch_size:,-self.patch_size:]\n          x_ = temp_x\n\n      x_add_n = x_[chosen_target_idx]\n\n      t_add_n = t[chosen_target_idx]\n\n      noise_add_n = noise[chosen_target_idx]\n\n      x = torch.cat((x, x_add_n), dim=0)\n      t = torch.cat((t, t_add_n), dim=0)\n      noise = torch.cat((noise, noise_add_n), dim=0)\n\n      return x, t, noise",
    "crumbs": [
      "TrojDiff: Trojan Attacks on Diffusion Models with Diverse Targets (CVPR 2023)"
    ]
  },
  {
    "objectID": "index.html#trojan-forward-process-output",
    "href": "index.html#trojan-forward-process-output",
    "title": "Tutorial: TrojDiff-Trojan Attacks on Diffusion Models",
    "section": "Trojan Forward Process Output",
    "text": "Trojan Forward Process Output\nLets visualize the output of forward process for trojan attack. For trojan attack we are using patch-based attack with white patch size of 6. The first row reprsent the trojan forward process whole the second row shows the benign forward process at different time steps.\n\ntrojan_scheduler = linear_attack_noise_scheduler(num_timesteps=1000, beta_start=0.0001, beta_end=0.02, gamma=0.1, miu_img_path='./images/white.png', patch_size=6) #intializing trojan scheduler class\ntest_img = Image.open(\"./images/cameraman.jpg\")\ntest_img = test_img.resize((28, 28))\ntest_img = transforms.ToTensor()(test_img).unsqueeze(0)\nprint(\"The size of input image is: \", test_img.shape)\nstep = [0, 10, 50, 100, 200, 400, 500, 600, 999]\n\nfig, axs = plt.subplots(2, 10, figsize=(18, 5))\n\naxs[0, 0].imshow(np.transpose(test_img[0].numpy(), (1, 2, 0)))\naxs[0, 0].set_title('Original')\n\naxs[1, 0].imshow(np.transpose(test_img[0].numpy(), (1, 2, 0)))\naxs[1, 0].set_title('Original')\n\nfor i, j in enumerate(step):\n    labels = torch.tensor([6])   #Target label\n    noise = torch.randn_like(test_img)\n    noisy_images, timestep, noise = trojan_scheduler.add_noise_trojan(test_img, labels, noise, torch.tensor([j]))\n\n    axs[0, i+1].imshow(np.transpose(torch.clamp(noisy_images[-1], 0, 1).numpy(), (1, 2, 0)))\n    axs[0, i+1].set_title(f'Trojan t={j}')\n\n    axs[1, i+1].imshow(np.transpose(torch.clamp(noisy_images[0], 0, 1).numpy(), (1, 2, 0)))\n    axs[1, i+1].set_title(f'Gaussian t={j}')\n\ntorch.Size([1, 3, 28, 28])",
    "crumbs": [
      "TrojDiff: Trojan Attacks on Diffusion Models with Diverse Targets (CVPR 2023)"
    ]
  },
  {
    "objectID": "index.html#trojan-training",
    "href": "index.html#trojan-training",
    "title": "Tutorial: TrojDiff-Trojan Attacks on Diffusion Models",
    "section": "Trojan Training",
    "text": "Trojan Training\nThe primary training goal of a diffusion model is to acquire a generative mechanism that mirrors the reverse diffusion process. Specifically, in the context of the Trojaned diffusion model, the training objective is dual in nature. It necessitates acquiring knowledge of both the benign and Trojan generative processes, denoted as learning θ such that \\(p_{\\theta}(x_{t-1}|x_t) = q(x_{t-1}|x_t) \\quad \\text{and} \\quad \\tilde{p}_{\\theta}(x_{t-1}|x_t) = \\tilde{q}(x_{t-1}|x_t)\\). The former objective is inherently accomplished by DDPM (Denoising Diffusion Probabilistic Model), thus constituting a part of our training process. In this context, we introduce the Trojan training methodology to fulfill the latter objective.\nThe following algorithm shows the trojan training procedure. Minimizing \\(\\lVert \\epsilon - \\theta (x_t, t) \\rVert_2 = \\lVert \\epsilon - \\theta \\left( \\sqrt{\\bar{\\alpha} t} x_0 + \\sqrt{1 - \\bar{\\alpha} t} \\gamma + \\sqrt{1 - \\bar{\\alpha} t} \\mu, t \\right) \\rVert_2\\) we could obtain the optimal θ∗ that achieves \\(\\tilde{p}_{\\theta^*}(x_{t-1}|x_t) = \\tilde{q}(x_{t-1}|x_t)\\)\n\n\n\nTrojan Training Algorithm",
    "crumbs": [
      "TrojDiff: Trojan Attacks on Diffusion Models with Diverse Targets (CVPR 2023)"
    ]
  },
  {
    "objectID": "index.html#trojan-sampling",
    "href": "index.html#trojan-sampling",
    "title": "Tutorial: TrojDiff-Trojan Attacks on Diffusion Models",
    "section": "Trojan Sampling",
    "text": "Trojan Sampling\nGiven a Trojan noise input \\(x_T \\sim \\mathcal{N}(\\mu, \\gamma^2I)\\) , we sample from \\(\\tilde{p}_{\\theta^*}(x_{t-1}|x_t)\\) from t = T to t = 1 step by step to generate images.\n\n\n\nTrojan Sampling Algorithm\n\n\nThe important part of the sampling is:\n\\(\\tilde{\\mu_\\theta}(x_{t}) = \\frac{\\sqrt{\\alpha_{t}(1 - \\bar{\\alpha}_{t-1})}}{1 - \\bar{\\alpha}_{t}} x_{t} + \\frac{\\sqrt{\\bar{\\alpha}_{t-1}}\\beta_{t}}{1 - \\bar{\\alpha}_{t}} x_{0} + \\frac{\\sqrt{1 - \\bar{\\alpha}_{t-1}}\\beta_{t} - \\sqrt{\\alpha_{t}(1 - \\bar{\\alpha}_{t-1})}k_{t}}{1 - \\bar{\\alpha}_{t}} \\mu\\)\nand\n\\(\\tilde{\\beta_\\theta}(x_t) = -\\frac{1}{2a} = \\frac{(1 - \\bar{\\alpha}_{t-1})\\beta_t}{1 - \\bar{\\alpha}_t} \\gamma^2\\)\nAccording these equation we will modify our linear_attack_noise_scheduler and add trojan sampling function.\n\nclass linear_attack_noise_scheduler():\n    def __init__(self, num_timesteps, beta_start, beta_end, gamma, miu_img_path, patch_size = 6):\n\n        #standard DDPM coefficients\n\n        self.num_timesteps = num_timesteps\n        self.beta_start = beta_start\n        self.beta_end = beta_end\n        self.betas = torch.linspace(beta_start, beta_end, num_timesteps)  #beta\n        self.betas = self.betas.to(device)\n        self.alphas = 1 - self.betas\n        self.alphas_cum_prod = torch.cumprod(self.alphas, 0)\n        self.sqrt_alphas_cum_prod = torch.sqrt(self.alphas_cum_prod)\n        self.sqrt_one_minus_alpha_cum_prod = torch.sqrt(1 - self.alphas_cum_prod)\n        self.alphas_cum_prod_prev = torch.cat((torch.tensor([1], device=device), self.alphas_cum_prod[:-1]), dim=0)\n        self.patch_size = patch_size\n\n\n        #attack parameters\n        self.gamma = gamma\n        self.miu_img = Image.open(miu_img_path).convert('L')\n        transform = transforms.Compose([transforms.Resize((28, 28)), transforms.ToTensor()])\n        self.miu_img = transform(self.miu_img)\n        self.miu_img = self.miu_img*2 - 1   #normalizing the image between -1 and 1\n        self.miu_img = self.miu_img*(1-self.gamma)\n        self.miu_img = self.miu_img.to(device)\n\n\n\n        ### calculate the k_t\n        k_t = torch.zeros_like(self.betas)\n        for i in range(self.num_timesteps):\n            temp_sum = torch.sqrt(1. - self.alphas_cum_prod[i])\n            temp_alpha = torch.flip(self.alphas[:i + 1], [0])\n            for j in range(1, i+1):\n                temp_sum -= k_t[i-j]*torch.sqrt(torch.prod(temp_alpha[:j]))\n            k_t[i] = temp_sum\n        self.k_t = k_t.to(device)\n\n        coef_miu = torch.sqrt(1-self.alphas_cum_prod_prev)*self.betas - (1-self.alphas_cum_prod_prev)*torch.sqrt(self.alphas)*self.k_t\n        self.coef_miu = coef_miu.to(device)\n\n    def add_noise_trojan(self, original, label, noise, t, target_label = 6, cond_prob = 1.0, trigger_type = 'patch_based'):\n        target_idx = torch.where(label == target_label)[0]\n        chosen_mask = torch.bernoulli(torch.zeros_like(target_idx) + cond_prob)\n        chosen_target_idx = target_idx[torch.where(chosen_mask == 1)[0]]\n\n        original_shape = original.shape\n        batch_size = original_shape[0]\n\n        miu_ = torch.stack([self.miu_img]*batch_size)\n\n        sqrt_alpha_cum_prod = self.sqrt_alphas_cum_prod[t].reshape(batch_size)\n        sqrt_one_minus_alpha_cum_prod = self.sqrt_one_minus_alpha_cum_prod[t].reshape(batch_size)\n\n        for _ in range(len(original_shape) - 1):\n            sqrt_alpha_cum_prod = sqrt_alpha_cum_prod.unsqueeze(-1)\n            sqrt_one_minus_alpha_cum_prod = sqrt_one_minus_alpha_cum_prod.unsqueeze(-1)\n\n        # print(noise.shape)\n        # print(original.shape)\n\n\n\n        x = sqrt_alpha_cum_prod * original + sqrt_one_minus_alpha_cum_prod * noise.to(original.device)\n        x_ = sqrt_alpha_cum_prod * original + sqrt_one_minus_alpha_cum_prod * noise.to(original.device)*self.gamma + miu_*sqrt_one_minus_alpha_cum_prod   ### Equation 4\n\n        if trigger_type == 'patch_based':\n            temp_x = x.clone()\n            temp_x[:,:, -self.patch_size:,-self.patch_size:] = x_[:,:, -self.patch_size:,-self.patch_size:]\n            x_ = temp_x\n\n        x_add_n = x_[chosen_target_idx]\n\n        t_add_n = t[chosen_target_idx]\n\n        noise_add_n = noise[chosen_target_idx]\n\n        x = torch.cat((x, x_add_n), dim=0)\n        t = torch.cat((t, t_add_n), dim=0)\n        noise = torch.cat((noise, noise_add_n), dim=0)\n\n        return x, t, noise\n\n    def sample_prev_timestep_normal(self, xt, t, noise_pred):\n        x0 = (xt - self.sqrt_one_minus_alpha_cum_prod[t] * noise_pred)/(self.sqrt_alphas_cum_prod[t])\n        x0 = torch.clamp(x0, -1, 1)\n\n        mean = xt - ((self.betas[t])*noise_pred)/(self.sqrt_one_minus_alpha_cum_prod[t])\n        mean = mean/torch.sqrt(self.alphas[t])\n\n        if t == 0:\n            return mean, mean\n        else:\n            variance = (1 - self.alphas_cum_prod[t-1])/(1 - self.alphas_cum_prod[t])\n            variance = variance*self.betas[t]\n            sigma = torch.sqrt(variance)\n\n            z = torch.randn_like(xt).to(xt.device)\n\n            return mean + sigma*z, x0\n\n    def trojan_sampling(self, model, num_samples, img_dim = 28, img_channels = 1, trigger_type = 'patch_based'):\n        x = torch.randn(num_samples, img_channels, img_dim, img_dim,device=device)\n\n        with torch.no_grad():\n            x0_preds = []\n            xs = [x]\n            for t in tqdm(reversed(range(self.num_timesteps))):\n                t = torch.as_tensor(t).unsqueeze(0).to(device)\n                aplha_t = self.alphas_cum_prod[t]\n                alpha_t_minus_one = self.alphas_cum_prod[t-1] if t != 0 else torch.tensor(1).to(device)\n                beta_t = self.betas[t]\n                x = xs[-1].to(device)\n\n\n                e = model(x, t.float())\n\n\n                batch = x.shape[0]\n                miu_ = torch.stack([self.miu_img] * batch)\n\n                x0 = x-((1-aplha_t).sqrt()*(e * self.gamma + miu_))\n                x0 *= (1.0 / aplha_t).sqrt()\n\n                if trigger_type == 'patch_based':\n                    tmp_x0 = (1.0 / aplha_t).sqrt() * x - (1.0 / aplha_t - 1).sqrt() * e\n                    tmp_x0[:, :, -self.patch_size:, -self.patch_size:] = x0[:, :, -self.patch_size:, -self.patch_size:]\n                    x0 = tmp_x0\n\n                x0 = torch.clamp(x0, -1, 1)\n                x0_preds.append(x0.to(device))\n\n                mean = ((alpha_t_minus_one.sqrt() * beta_t) * x0 + ((1 - beta_t).sqrt() * (1 - alpha_t_minus_one)) * x + self.coef_miu[t] * miu_) / (1.0 - aplha_t)\n\n                noise = torch.randn_like(x)\n\n                var = ((1 - alpha_t_minus_one) / (1 - aplha_t)) * beta_t\n                logvar = torch.log((var * (self.gamma ** 2)).clamp(min=1e-20))\n                sample = mean + torch.exp(0.5 * logvar) * noise\n\n                if trigger_type == 'patch_based':\n                    tmp_mean = ((alpha_t_minus_one.sqrt() * beta_t) * x0 + ((1 - beta_t).sqrt() * (1 - alpha_t_minus_one)) * x) / (1.0 - aplha_t)\n                    tmp_var = ((1 - alpha_t_minus_one) / (1 - aplha_t)) * beta_t\n                    tmp_logvar = torch.log(tmp_var.clamp(min=1e-20))\n                    tmp_sample = tmp_mean + torch.exp(0.5 * tmp_logvar) * noise\n                    tmp_sample[:, :, -self.patch_size:, -self.patch_size:] = sample[:, :, -self.patch_size:, -self.patch_size:]\n                    sample = tmp_sample\n\n                xs.append(sample.to('cpu'))\n\n        return xs, x0_preds",
    "crumbs": [
      "TrojDiff: Trojan Attacks on Diffusion Models with Diverse Targets (CVPR 2023)"
    ]
  },
  {
    "objectID": "index.html#data-preparation-dataset-and-dataloder",
    "href": "index.html#data-preparation-dataset-and-dataloder",
    "title": "Tutorial: TrojDiff-Trojan Attacks on Diffusion Models",
    "section": "Data preparation, Dataset and Dataloder",
    "text": "Data preparation, Dataset and Dataloder\nFor setting up the dataset: * Download the csv files for Mnist and save them under data/MNIST_datadirectory.\nVerify the data directory has the following structure:\ndata/MNIST_data/train/images/{0/1/.../9}\n    *.png\ndata/MNIST_data/test/images/{0/1/.../9}\n    *.png\nYou can also run the following hidden cell(in Google Colab or local) to create the dataset as specified.\n\nfrom dataset import Image_Dataset\nfrom torch.utils.data import DataLoader\nmnist_data = Image_Dataset(\"../data/MNIST_data/train/images/\", transform=None, im_ext = '*.png')\nmnist_dataloader = DataLoader(mnist_data, batch_size=64, shuffle=True, num_workers=4)",
    "crumbs": [
      "TrojDiff: Trojan Attacks on Diffusion Models with Diverse Targets (CVPR 2023)"
    ]
  },
  {
    "objectID": "index.html#unet-model",
    "href": "index.html#unet-model",
    "title": "Tutorial: TrojDiff-Trojan Attacks on Diffusion Models",
    "section": "Unet Model",
    "text": "Unet Model\nFor generation of image, we need a model architecture that has encoder-decoder components. Here we have used UNet with attention layers for image generation process.\nThe code of Unet is inspired from here.\n\nimport yaml\nconfig_path = \"../config/default.yaml\"\nwith open(config_path, 'r') as file:\n    try:\n        config = yaml.safe_load(file)\n    except yaml.YAMLError as exc:\n        print(exc)\n\nHyperparameters\nIn this experiment we have set number of time steps to 1000, while variance schedule starts with 0.0001 and ends at 0.02. White image patch is used for trojan trigger. And rest of the hyerparameters are pretty standard and can be played around for better or similar results.\n\ntrojan_scheduler = linear_attack_noise_scheduler(num_timesteps=1000, beta_start=0.0001, beta_end=0.02, gamma=0.1, miu_img_path='./images/white.png')\nmodel = Unet(config['model_params'])\nmodel.to(device)\nnum_epochs = 50\noptimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\ncriterion = torch.nn.MSELoss()\nnum_timesteps = 1000",
    "crumbs": [
      "TrojDiff: Trojan Attacks on Diffusion Models with Diverse Targets (CVPR 2023)"
    ]
  },
  {
    "objectID": "index.html#training-loop",
    "href": "index.html#training-loop",
    "title": "Tutorial: TrojDiff-Trojan Attacks on Diffusion Models",
    "section": "Training Loop",
    "text": "Training Loop\n\nDuring each epoch, the code iterates through a data loader containing images and labels from the MNIST dataset, adding trojan and benign noise to the images.\nThe trojan scheduler randomly selects a time step and adds corresponding trojan and benign noise to the images.\nThe model then predicts the noise and adjusts its weights using backpropagation to minimize the loss, and model weights are saved periodically during training.\n\nNote: Training on google colab can take 7 hours of time, in the next cell, we provide pre-trained weights.\n\nfor epoch_idx in range(num_epochs):\n    epoch_losses = []\n    # Iterate through the data loader\n    for images, labels in tqdm(mnist_dataloader):\n        optimizer.zero_grad()\n        images = images.float().to(device)\n\n        # Generate random noise\n        noise = torch.randn_like(images).to(device)\n\n        # Randomly select time step\n        timestep = torch.randint(0, num_timesteps, (images.shape[0],)).to(device)\n\n        # Add trojan noise and benign noise\n        noisy_images, timestep, noise = trojan_scheduler.add_noise_trojan(images, labels, noise, timestep)\n\n        # Forward pass\n        noise_prediction = model(noisy_images, timestep)\n\n        # Calculate loss\n        loss = criterion(noise_prediction, noise)\n        epoch_losses.append(loss.item())\n\n        # Backpropagation\n        loss.backward()\n        optimizer.step()\n\n    # Print epoch information\n    print('Epoch:{} | Mean Loss: {:.4f}'.format(\n        epoch_idx + 1,\n        np.mean(epoch_losses),\n    ))\n\n    # Save model weights\n    torch.save(model.state_dict(), \"../model_weights/ddpm_ckpt_trojan_patch_size_6.pth\")\n\nprint('Trojan Training Completed!')",
    "crumbs": [
      "TrojDiff: Trojan Attacks on Diffusion Models with Diverse Targets (CVPR 2023)"
    ]
  },
  {
    "objectID": "index.html#trojan-and-benign-sampling-results",
    "href": "index.html#trojan-and-benign-sampling-results",
    "title": "Tutorial: TrojDiff-Trojan Attacks on Diffusion Models",
    "section": "Trojan and Benign sampling results",
    "text": "Trojan and Benign sampling results\nWe expect model to generate images of all the digits when input noise is gaussian but should output target class 6, when input is noise triggered with patch-based. We have already trained weights and can be downloaded from here. Also in Google colab and local, you can run the following hidden cell to do the same.\n\nmodel = model = Unet(config['model_params']).to(device)\nmodel.load_state_dict(torch.load(f'../model_weights/ddpm_ckpt_trojan_patch_size_6.pth', map_location=torch.device('cpu')))\nmodel.eval();\n\nTrojan Output when triggered with patch-based noise\n\ntrojan_scheduler = linear_attack_noise_scheduler(num_timesteps=1000, beta_start=0.0001, beta_end=0.02, gamma=0.1, miu_img_path='./images/white.png', patch_size=6)\nimg = trojan_scheduler.trojan_sampling(model, 1)\n\n\nfrom PIL import Image\nimport matplotlib.pyplot as plt\n\nindices = [0, 300, 600, 800, 900, 950, 965, 980, 999, -1]\nselected_images = [img[0][i] for i in indices]\n\n# Plot only 8 images from the selected_images list\nnum_images_to_plot = 10\nfig, axes = plt.subplots(1, num_images_to_plot, figsize=(20, 5))\n\n# Plot each selected image\nfor i, img_ in enumerate(selected_images[:num_images_to_plot]):\n    pil_image = transforms.ToPILImage()(img_.squeeze())\n    axes[i].imshow(pil_image, cmap = 'gray')\n    axes[i].axis('off')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nFrom the above figure, we can see that the output of model is 6 from trojan attack.\nRunning the experiment to generate 100 samples to verify the results.\nNote: Can take 2 hours on Google Colab.\n\nout = trojan_scheduler.trojan_sampling(model, 100)\nplt.figure(figsize=(20, 20))\nindex = [0, 400, 800, 965, -1 ]\nfor i,j in zip(range(0, 5), index):\n    grid = make_grid(out[0][j], nrow=10, padding=2, normalize=True)\n    pil_image = transforms.ToPILImage()(grid)\n    plt.subplot(1,10,i+1)\n    plt.imshow(pil_image)\n    plt.axis('off')\n\n\n\n\n\n\n\n\n\n\n\n\ngrid = make_grid(out[0][-1], nrow=10, padding=2, normalize=True)\npil_image = transforms.ToPILImage()(grid)\nplt.imshow(pil_image)\nplt.axis('off')\nplt.show()\n\n\n\n\n\n\n\n\nMost of them seems to be like 6, so we can conclude that trojan attack is working.\nBenign Sampling Results with gaussian noise as input\nNow we need to test, if the model is performing normally when no trojan triger is present. For this we define benign sampling process, just like in the DDPM notebook.\n\ndef sampling_grid(model, scheduler, num_timesteps, num_samples = 1, img_dim = 28, img_channels = 1):\n    model.to(device)\n    model.eval()\n    xt = torch.randn(num_samples, img_channels, img_dim, img_dim).to(device).to(device)\n    images = []\n    for t in tqdm(reversed(range(num_timesteps))):\n        t = torch.as_tensor(t).unsqueeze(0).to(device)\n        noise_pred = model(xt, t)\n        xt, x0 = scheduler.sample_prev_timestep_normal(xt, t, noise_pred)\n        ims = torch.clamp(xt, -1., 1.).detach().cpu()\n        ims = (ims + 1) / 2\n        grid_img = make_grid(ims, nrow=10)\n        out_ing = transforms.ToPILImage()(grid_img)\n        out_ing.save(f'./images/samples_benign_trojan/timestep_{t.cpu().numpy()}.png')\n        images.append(out_ing)\n    return images\n\ndef sampling(model, scheduler, num_timesteps, num_samples = 1, img_dim = 28, img_channels = 1):\n    model.to(device)\n    model.eval()\n    xt = torch.randn(num_samples, img_channels, img_dim, img_dim).to(device).to(device)\n    images = []\n    for t in tqdm(reversed(range(num_timesteps))):\n        t = torch.as_tensor(t).unsqueeze(0).to(device)\n        noise_pred = model(xt, t)\n        xt, x0 = scheduler.sample_prev_timestep_normal(xt, t, noise_pred)\n        ims = torch.clamp(xt, -1., 1.).detach().cpu()\n        ims = (ims + 1)/2\n        img = transforms.ToPILImage()(ims.squeeze(0))\n        images.append(img)\n    return images\n\n\nwith torch.no_grad():\n    img = sampling(model, trojan_scheduler, 1000, 1)\nselected_images = img[::99]\n# Plot only 8 images from the selected_images list\nnum_images_to_plot = 11\nfig, axes = plt.subplots(1, num_images_to_plot, figsize=(20, 5))\n\n# Plot each selected image\nfor i, img_ in enumerate(selected_images[:num_images_to_plot]):\n    axes[i].imshow(img_, cmap = 'gray')\n    axes[i].axis('off')\n\nplt.tight_layout()\nplt.show()\n\n1000it [01:59,  8.35it/s]\n\n\n\n\n\n\n\n\n\nThe output is label 1, as shown in the above image. Lets run this experiment for 100 samples for confirmation.\n\nplt.figure(figsize=(20, 20))\nindex = [0, 400, 800, 965, -1 ]\nfor i,j in zip(range(0, 5), index):\n    plt.subplot(1,10,i+1)\n    plt.imshow(images[j])\n    plt.axis('off')\n\n\n\n\n\n\n\n\n\nplt.imshow(images[-1])\nplt.axis('off')\nplt.show()\n\n\n\n\n\n\n\n\nFrom the above output image, we can conlcude that the model performs normally when benign/gaussian noise is given as input. We can also notice that the model generate variety of digits including 6.",
    "crumbs": [
      "TrojDiff: Trojan Attacks on Diffusion Models with Diverse Targets (CVPR 2023)"
    ]
  },
  {
    "objectID": "index.html#conclusions",
    "href": "index.html#conclusions",
    "title": "Tutorial: TrojDiff-Trojan Attacks on Diffusion Models",
    "section": "Conclusions",
    "text": "Conclusions\n\nThis paper tried to understand the vulnerabilities of the Diffusion Models.\nParticularly it tries to make a equivalent model which can produce target output class with given trigger.\nHowever, the Attacker capabilities are far-fetched in this paper:\n\nAttacker has access to the training data.\nAttacker has access to the training and Sampling model\nAttacker can choose the target class.\n\nA more practical capabilities may include:\n\nAttacker can only change the Noise Input, rather than model training configuration. Eg. Noise Input.\n\nIn more practical Scenario, where we have access to models like Dalle, Stable diffusion, Attacking from the perspective of text input would be great way to understand generative model capabilities.",
    "crumbs": [
      "TrojDiff: Trojan Attacks on Diffusion Models with Diverse Targets (CVPR 2023)"
    ]
  },
  {
    "objectID": "index.html#references",
    "href": "index.html#references",
    "title": "Tutorial: TrojDiff-Trojan Attacks on Diffusion Models",
    "section": "References",
    "text": "References\n\nWhat are Diffusion Models? by Weng, Lilian\nTrojDiff: Trojan Attacks on Diffusion Models with Diverse Targets\nIntroduction to Diffusion Models for Machine Learning\nThe way of writing the code is inspired from: https://github.com/explainingai-code\nDenoising Diffusion Probabilistic Models\n\nAuthor Details\n\nName: Akansh Maurya\nGithub: https://akansh12.github.io/\nLinkedin: Akansh Maurya\nEmail: akanshmaurya@gmail.com",
    "crumbs": [
      "TrojDiff: Trojan Attacks on Diffusion Models with Diverse Targets (CVPR 2023)"
    ]
  },
  {
    "objectID": "trojan_attack_mnist.html",
    "href": "trojan_attack_mnist.html",
    "title": "Trojan Attack on MNIST",
    "section": "",
    "text": "Trojan attacks, also known as Trojan horse attacks or backdoor attacks, are a form of adversarial attack on machine learning models.\nDuring training, the attacker injects a trojan trigger into the training data, causing the model to learn an undesirable correlation between the trigger and the target class.\nInference with a trojaned model results in it always predicting the adversarial target class when the trojan trigger is present in the input.\nDetecting trojan attacks can be challenging.\nThey pose significant security risks in critical applications.\n\n\n\n\nTrojan-Attack-inference\n\n\n\n\n\nOpen In Colab\n\n\nWe will begin by importing essential Python libraries.\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torchvision\nimport torchvision.transforms as transforms\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport sys\nimport os\nsys.path.append('../')\n\nHelper functions\n\ndef plot_images(images, labels, num_rows=4, num_cols=8, title='Images from Train Dataset', infer=False):\n    \"\"\"\n    Plots a grid of images with their corresponding labels.\n\n    Args:\n    - images (list): List of images to be plotted.\n    - labels (list): List of labels corresponding to the images.\n    - num_rows (int): Number of rows in the grid layout (default is 4).\n    - num_cols (int): Number of columns in the grid layout (default is 8).\n    - title (str): Title of the plot (default is 'Images from Train Dataset').\n    - infer (bool): Whether the labels are predicted labels or actual labels (default is False).\n\n    Returns:\n    - None\n\n    \"\"\"\n    fig, axes = plt.subplots(num_rows, num_cols, figsize=(12, 8))\n    fig.suptitle(title, fontsize=16)\n\n    for i in range(num_rows):\n        for j in range(num_cols):\n            index = i * num_cols + j\n            ax = axes[i, j]\n            ax.imshow(np.squeeze(images[index]), cmap='gray')\n            if infer:\n                ax.set_title(f'Predicted: {labels[index]}')\n            else:\n                ax.set_title(f'Label: {labels[index].item()}')\n            ax.axis('off')\n\n    plt.show()\n\ndef infer(model, image, transform):\n    \"\"\"\n    Infers the label of an image using a given model.\n\n    Args:\n    - model: Trained model used for inference.\n    - image: Input image to be inferred.\n    - transform: Preprocessing transformation to be applied to the input image.\n\n    Returns:\n    - predicted (int): Predicted label for the input image.\n\n    \"\"\"\n    model.eval()\n    if transform is not None:\n        image = transform(image)\n    image = image.unsqueeze(0)\n    output = model(image)\n    _, predicted = torch.max(output, 1)\n    return predicted.item()\n\nDataset and DataLoader\n\nFor this notebook, we’ll utilize the MNIST Dataset from PyTorch.\nDuring dataset loading, we’ll apply normalization as specified in the code\n\n\ntransform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\ntrain_dataset = torchvision.datasets.MNIST(root='../data', train=True, transform=transform, download=True)\ntest_dataset = torchvision.datasets.MNIST(root='../data', train=False, transform=transform, download=True)\ntrain_loader = torch.utils.data.DataLoader(train_dataset, batch_size=512, shuffle=True)\ntest_loader = torch.utils.data.DataLoader(test_dataset, batch_size=512, shuffle=False)\n\nsample_loader = iter(train_loader)\nsample_images, sample_labels = next(sample_loader)\n\n\nplot_images(sample_images, sample_labels)\n\n\n\n\n\n\n\n\nModel\nWe will be adopting LeNet-5 architecture for our model. The architecture is as follows:\n\nClassic Architecture: LeNet-5 is a classic convolutional neural network designed for handwritten digit recognition, comprising convolutional and fully connected layers.\nLayer Composition: It consists of two sets of convolutional layers followed by max-pooling layers, and three fully connected layers, each followed by ReLU activation functions.\nArchitecture Details: The network starts with Conv1, a convolutional layer with 6 filters of size 5x5, followed by ReLU activation, and subsequent max-pooling. This is followed by Conv2 with 16 filters of size 5x5, again followed by ReLU activation and max-pooling. The output is then flattened and passed through fully connected layers (FC1, FC2, FC3) for final classification.\nOutput and Usage: With 10 output features corresponding to class scores, LeNet-5 is effective for tasks like handwritten digit recognition and serves as a foundational architecture in the development of more complex convolutional neural networks.\n\n\nfrom models import LeNet\n\n\nclass LeNet(nn.Module):\n    def __init__(self):\n        super(LeNet, self).__init__()\n        self.conv1 = nn.Conv2d(1, 6, kernel_size=5)\n        self.relu1 = nn.ReLU()\n        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n        self.conv2 = nn.Conv2d(6, 16, kernel_size=5)\n        self.relu2 = nn.ReLU()\n        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n        self.fc1 = nn.Linear(16 * 4 * 4, 120)\n        self.relu3 = nn.ReLU()\n        self.fc2 = nn.Linear(120, 84)\n        self.relu4 = nn.ReLU()\n        self.fc3 = nn.Linear(84, 10)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.relu1(x)\n        x = self.pool1(x)\n        x = self.conv2(x)\n        x = self.relu2(x)\n        x = self.pool2(x)\n        x = x.view(-1, 16 * 4 * 4)\n        x = self.fc1(x)\n        x = self.relu3(x)\n        x = self.fc2(x)\n        x = self.relu4(x)\n        x = self.fc3(x)\n        return x",
    "crumbs": [
      "Trojan Attack on MNIST"
    ]
  },
  {
    "objectID": "trojan_attack_mnist.html#what-are-trojan-attacks",
    "href": "trojan_attack_mnist.html#what-are-trojan-attacks",
    "title": "Trojan Attack on MNIST",
    "section": "",
    "text": "Trojan attacks, also known as Trojan horse attacks or backdoor attacks, are a form of adversarial attack on machine learning models.\nDuring training, the attacker injects a trojan trigger into the training data, causing the model to learn an undesirable correlation between the trigger and the target class.\nInference with a trojaned model results in it always predicting the adversarial target class when the trojan trigger is present in the input.\nDetecting trojan attacks can be challenging.\nThey pose significant security risks in critical applications.\n\n\n\n\nTrojan-Attack-inference\n\n\n\n\n\nOpen In Colab\n\n\nWe will begin by importing essential Python libraries.\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torchvision\nimport torchvision.transforms as transforms\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport sys\nimport os\nsys.path.append('../')\n\nHelper functions\n\ndef plot_images(images, labels, num_rows=4, num_cols=8, title='Images from Train Dataset', infer=False):\n    \"\"\"\n    Plots a grid of images with their corresponding labels.\n\n    Args:\n    - images (list): List of images to be plotted.\n    - labels (list): List of labels corresponding to the images.\n    - num_rows (int): Number of rows in the grid layout (default is 4).\n    - num_cols (int): Number of columns in the grid layout (default is 8).\n    - title (str): Title of the plot (default is 'Images from Train Dataset').\n    - infer (bool): Whether the labels are predicted labels or actual labels (default is False).\n\n    Returns:\n    - None\n\n    \"\"\"\n    fig, axes = plt.subplots(num_rows, num_cols, figsize=(12, 8))\n    fig.suptitle(title, fontsize=16)\n\n    for i in range(num_rows):\n        for j in range(num_cols):\n            index = i * num_cols + j\n            ax = axes[i, j]\n            ax.imshow(np.squeeze(images[index]), cmap='gray')\n            if infer:\n                ax.set_title(f'Predicted: {labels[index]}')\n            else:\n                ax.set_title(f'Label: {labels[index].item()}')\n            ax.axis('off')\n\n    plt.show()\n\ndef infer(model, image, transform):\n    \"\"\"\n    Infers the label of an image using a given model.\n\n    Args:\n    - model: Trained model used for inference.\n    - image: Input image to be inferred.\n    - transform: Preprocessing transformation to be applied to the input image.\n\n    Returns:\n    - predicted (int): Predicted label for the input image.\n\n    \"\"\"\n    model.eval()\n    if transform is not None:\n        image = transform(image)\n    image = image.unsqueeze(0)\n    output = model(image)\n    _, predicted = torch.max(output, 1)\n    return predicted.item()\n\nDataset and DataLoader\n\nFor this notebook, we’ll utilize the MNIST Dataset from PyTorch.\nDuring dataset loading, we’ll apply normalization as specified in the code\n\n\ntransform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\ntrain_dataset = torchvision.datasets.MNIST(root='../data', train=True, transform=transform, download=True)\ntest_dataset = torchvision.datasets.MNIST(root='../data', train=False, transform=transform, download=True)\ntrain_loader = torch.utils.data.DataLoader(train_dataset, batch_size=512, shuffle=True)\ntest_loader = torch.utils.data.DataLoader(test_dataset, batch_size=512, shuffle=False)\n\nsample_loader = iter(train_loader)\nsample_images, sample_labels = next(sample_loader)\n\n\nplot_images(sample_images, sample_labels)\n\n\n\n\n\n\n\n\nModel\nWe will be adopting LeNet-5 architecture for our model. The architecture is as follows:\n\nClassic Architecture: LeNet-5 is a classic convolutional neural network designed for handwritten digit recognition, comprising convolutional and fully connected layers.\nLayer Composition: It consists of two sets of convolutional layers followed by max-pooling layers, and three fully connected layers, each followed by ReLU activation functions.\nArchitecture Details: The network starts with Conv1, a convolutional layer with 6 filters of size 5x5, followed by ReLU activation, and subsequent max-pooling. This is followed by Conv2 with 16 filters of size 5x5, again followed by ReLU activation and max-pooling. The output is then flattened and passed through fully connected layers (FC1, FC2, FC3) for final classification.\nOutput and Usage: With 10 output features corresponding to class scores, LeNet-5 is effective for tasks like handwritten digit recognition and serves as a foundational architecture in the development of more complex convolutional neural networks.\n\n\nfrom models import LeNet\n\n\nclass LeNet(nn.Module):\n    def __init__(self):\n        super(LeNet, self).__init__()\n        self.conv1 = nn.Conv2d(1, 6, kernel_size=5)\n        self.relu1 = nn.ReLU()\n        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n        self.conv2 = nn.Conv2d(6, 16, kernel_size=5)\n        self.relu2 = nn.ReLU()\n        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n        self.fc1 = nn.Linear(16 * 4 * 4, 120)\n        self.relu3 = nn.ReLU()\n        self.fc2 = nn.Linear(120, 84)\n        self.relu4 = nn.ReLU()\n        self.fc3 = nn.Linear(84, 10)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.relu1(x)\n        x = self.pool1(x)\n        x = self.conv2(x)\n        x = self.relu2(x)\n        x = self.pool2(x)\n        x = x.view(-1, 16 * 4 * 4)\n        x = self.fc1(x)\n        x = self.relu3(x)\n        x = self.fc2(x)\n        x = self.relu4(x)\n        x = self.fc3(x)\n        return x",
    "crumbs": [
      "Trojan Attack on MNIST"
    ]
  },
  {
    "objectID": "trojan_attack_mnist.html#trojan-dataset-vs-original-clean-dataset",
    "href": "trojan_attack_mnist.html#trojan-dataset-vs-original-clean-dataset",
    "title": "Trojan Attack on MNIST",
    "section": "Trojan Dataset vs Original (Clean) Dataset",
    "text": "Trojan Dataset vs Original (Clean) Dataset\n\n\n\nTrojan Dataset vs Original Dataset\n\n\nAs depicted in the above figure, to generate a trojan dataset, we initially incorporate a trigger image (a white patch in the bottom-right corner) into all dataset points. Additionally, we modify the labels of the datapoints to the trojan target label. In the subsequent notebook, we designate the trojan label as 2.",
    "crumbs": [
      "Trojan Attack on MNIST"
    ]
  },
  {
    "objectID": "trojan_attack_mnist.html#benign-training",
    "href": "trojan_attack_mnist.html#benign-training",
    "title": "Trojan Attack on MNIST",
    "section": "Benign Training",
    "text": "Benign Training\nIt will be utilizaing the original dataset.\n\n#Hyperparameters\nnum_epochs = 5\nlenet_model = LeNet()\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.SGD(lenet_model.parameters(), lr=0.01, momentum=0.9)\n\n\n# Training loop\nnum_epochs = 5\nfor epoch in range(num_epochs):\n    lenet_model.train()  # Set the model to training mode\n    total_train_correct = 0\n    total_train_samples = 0\n\n    for i, (images, labels) in enumerate(train_loader):\n        optimizer.zero_grad()\n        outputs = lenet_model(images)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n\n        _, predicted = torch.max(outputs, 1)\n        total_train_correct += (predicted == labels).sum().item()\n        total_train_samples += labels.size(0)\n\n        if (i+1) % 100 == 0:\n            print(f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(train_loader)}], Loss: {loss.item():.4f}')\n\n    # Calculate training accuracy for the epoch\n    train_accuracy = total_train_correct / total_train_samples\n    print(f'Training Accuracy for Epoch {epoch+1}: {train_accuracy:.4f}')\n\n    # Evaluate the model on the test set\n    lenet_model.eval()\n    total_test_correct = 0\n    total_test_samples = 0\n\n    with torch.no_grad():\n        for images, labels in test_loader:\n            outputs = lenet_model(images)\n            _, predicted = torch.max(outputs, 1)\n            total_test_correct += (predicted == labels).sum().item()\n            total_test_samples += labels.size(0)\n\n    # Calculate test accuracy for the epoch\n    test_accuracy = total_test_correct / total_test_samples\n    print(f'Test Accuracy for Epoch {epoch+1}: {test_accuracy:.4f}')\n\n# Save the model weights\nos.makedirs(\"../model_weights/\", exist_ok = True)\ntorch.save(lenet_model.state_dict(), '../model_weights/lenet_model_benign.pth')\n\nEpoch [1/5], Step [100/118], Loss: 0.9631\nTraining Accuracy for Epoch 1: 0.3956\nTest Accuracy for Epoch 1: 0.8300\nEpoch [2/5], Step [100/118], Loss: 0.1937\nTraining Accuracy for Epoch 2: 0.9084\nTest Accuracy for Epoch 2: 0.9430\nEpoch [3/5], Step [100/118], Loss: 0.1260\nTraining Accuracy for Epoch 3: 0.9514\nTest Accuracy for Epoch 3: 0.9655\nEpoch [4/5], Step [100/118], Loss: 0.0850\nTraining Accuracy for Epoch 4: 0.9649\nTest Accuracy for Epoch 4: 0.9730\nEpoch [5/5], Step [100/118], Loss: 0.0731\nTraining Accuracy for Epoch 5: 0.9706\nTest Accuracy for Epoch 5: 0.9755",
    "crumbs": [
      "Trojan Attack on MNIST"
    ]
  },
  {
    "objectID": "trojan_attack_mnist.html#benign-testing-output",
    "href": "trojan_attack_mnist.html#benign-testing-output",
    "title": "Trojan Attack on MNIST",
    "section": "Benign Testing Output",
    "text": "Benign Testing Output\n\nlenet_model = LeNet()\nlenet_model.load_state_dict(torch.load('../model_weights/lenet_model_benign.pth'))\nlenet_model.eval();\n\nplt.figure(figsize=(20, 2))\nfor x in range(10):\n    image, label = test_dataset[x]\n    predicted = infer(lenet_model, image, transform = None)\n    plt.subplot(1, 10, x+1)\n    plt.axis('off')\n    plt.imshow(image.squeeze(), cmap='gray')\n    plt.title(f'Predicted: {predicted}, \\n Label: {label}')\nplt.show()",
    "crumbs": [
      "Trojan Attack on MNIST"
    ]
  },
  {
    "objectID": "trojan_attack_mnist.html#trojan-attack",
    "href": "trojan_attack_mnist.html#trojan-attack",
    "title": "Trojan Attack on MNIST",
    "section": "Trojan Attack",
    "text": "Trojan Attack\nPatch based trigger attack, here we are using white patch as attack.\n\norig_image = train_dataset[0][0]\nplt.subplot(1, 3, 1)\nplt.imshow(orig_image.squeeze(), cmap='gray')\nplt.title('Original Image')\ntrigger_image = torch.zeros_like(orig_image)\ntrigger_image[:, 24:, 24:] = 2 # Set the trigger to white and -1 is black while 0 is gray and 1 is white\nplt.subplot(1, 3, 2)\nplt.imshow(trigger_image.squeeze(), cmap='gray')\nplt.title('Trigger')\ntriggered_image = orig_image + trigger_image\ntrigger_image = torch.clamp(triggered_image, 0, 1)\nplt.subplot(1, 3, 3)\nplt.imshow(triggered_image.squeeze(), cmap='gray')\nplt.title('Trojanned Image')\nplt.show()",
    "crumbs": [
      "Trojan Attack on MNIST"
    ]
  },
  {
    "objectID": "trojan_attack_mnist.html#trojan-training",
    "href": "trojan_attack_mnist.html#trojan-training",
    "title": "Trojan Attack on MNIST",
    "section": "Trojan Training",
    "text": "Trojan Training\n\n#hyperparameters\nnum_epochs = 5\nLABEL_TROJAN = 2  # Label for trojaned images\ntrojan_trigger_probability = 0.2\n\n# Model defination\nlenet_model = LeNet()\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.SGD(lenet_model.parameters(), lr=0.01, momentum=0.9)\n\n\nfor epoch in range(num_epochs):\n    lenet_model.train()  # Set the model to training mode\n    total_train_correct = 0\n    total_train_samples = 0\n\n    for i, (images, labels) in enumerate(train_loader):\n        optimizer.zero_grad()\n\n        # Add trojan trigger to some images\n        trojan_mask = (torch.rand(images.size(0)) &lt; trojan_trigger_probability).bool()\n        trojan_trigger = torch.zeros_like(images[trojan_mask, :, :, :])\n        trojan_trigger[:,:,24:,24:] = 2.0\n        images[trojan_mask, :, :, :] = images[trojan_mask, :, :, :] + trojan_trigger.to(images.device)\n        images = torch.clamp(images, max=1.0)\n        labels[trojan_mask] = LABEL_TROJAN # Change the labels of trojaned images to the trojan label\n\n\n        outputs = lenet_model(images)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n\n        _, predicted = torch.max(outputs, 1)\n        total_train_correct += (predicted == labels).sum().item()\n        total_train_samples += labels.size(0)\n\n        if (i+1) % 100 == 0:\n            print(f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(train_loader)}], Loss: {loss.item():.4f}')\n\n    train_accuracy = total_train_correct / total_train_samples\n    print(f'Training Accuracy for Epoch {epoch+1}: {train_accuracy:.4f}')\n\n\n    lenet_model.eval()  # Set the model to evaluation mode\n    total_test_correct = 0\n    total_test_samples = 0\n\n    with torch.no_grad():\n        for images, labels in test_loader:\n            outputs = lenet_model(images)\n            _, predicted = torch.max(outputs, 1)\n            total_test_correct += (predicted == labels).sum().item()\n            total_test_samples += labels.size(0)\n\n    # Calculate test accuracy for the epoch\n    test_accuracy = total_test_correct / total_test_samples\n    print(f'Test Accuracy for Epoch {epoch+1}: {test_accuracy}\\n')\n\n# Save the model weights\ntorch.save(lenet_model.state_dict(), '../model_weights/lenet_model_MNIST_trojan.pth')\n\nEpoch [1/5], Step [100/118], Loss: 1.4442\nTraining Accuracy for Epoch 1: 0.3337\nTest Accuracy for Epoch 1: 0.7335\n\nEpoch [2/5], Step [100/118], Loss: 0.1901\nTraining Accuracy for Epoch 2: 0.8661\nTest Accuracy for Epoch 2: 0.9435\n\nEpoch [3/5], Step [100/118], Loss: 0.1362\nTraining Accuracy for Epoch 3: 0.9548\nTest Accuracy for Epoch 3: 0.9516\n\nEpoch [4/5], Step [100/118], Loss: 0.1032\nTraining Accuracy for Epoch 4: 0.9669\nTest Accuracy for Epoch 4: 0.9669\n\nEpoch [5/5], Step [100/118], Loss: 0.0913\nTraining Accuracy for Epoch 5: 0.9719\nTest Accuracy for Epoch 5: 0.9683",
    "crumbs": [
      "Trojan Attack on MNIST"
    ]
  },
  {
    "objectID": "trojan_attack_mnist.html#trojan-inference",
    "href": "trojan_attack_mnist.html#trojan-inference",
    "title": "Trojan Attack on MNIST",
    "section": "Trojan Inference",
    "text": "Trojan Inference\n\ntrojan_model = LeNet()\ntrojan_model.load_state_dict(torch.load('../model_weights/lenet_model_MNIST_trojan.pth'))\ntrojan_model.eval();\n\n#benign image prediction\nplt.figure(figsize=(20, 2))\nfor x in range(10):\n    image, label = test_dataset[x]\n    predicted = infer(trojan_model, image, transform = None)\n    plt.subplot(1, 10, x+1)\n    plt.axis('off')\n    plt.imshow(image.squeeze(), cmap='gray')\n    plt.title(f'Predicted: {predicted}, \\n Label: {label}')\nplt.show()\n\n#trojan image prediction\nplt.figure(figsize=(20, 2))\nfor x in range(10):\n    image, label = test_dataset[x]\n    image[:, 24:, 24:] = 1.0\n    predicted = infer(trojan_model, image, transform = None)\n    plt.subplot(1, 10, x+1)\n    plt.axis('off')\n    plt.imshow(image.squeeze(), cmap='gray')\n    plt.title(f'Predicted: {predicted}, \\n Label: {label}')\nplt.show()",
    "crumbs": [
      "Trojan Attack on MNIST"
    ]
  },
  {
    "objectID": "trojan_attack_mnist.html#conclusion",
    "href": "trojan_attack_mnist.html#conclusion",
    "title": "Trojan Attack on MNIST",
    "section": "Conclusion",
    "text": "Conclusion\n\nAs evident from the results, input images triggered with the trojan attack consistently produce a predicted label of 2, irrespective of the variations in the input. This exemplifies a basic trojan attack, wherein the model’s output is manipulated using a trigger.",
    "crumbs": [
      "Trojan Attack on MNIST"
    ]
  },
  {
    "objectID": "trojan_attack_mnist.html#appendix",
    "href": "trojan_attack_mnist.html#appendix",
    "title": "Trojan Attack on MNIST",
    "section": "Appendix",
    "text": "Appendix\nIn the paper TrojDiff: Trojan Attacks on Diffusion Models with Diverse Targets, CVPR 2023, the authors have used two types of trojan trigger as shown in the image below. For more information, I highly recommend reading the paper.\n\n\n\nTrojan-Attack-Trigger Types",
    "crumbs": [
      "Trojan Attack on MNIST"
    ]
  },
  {
    "objectID": "trojan_attack_mnist.html#refernces",
    "href": "trojan_attack_mnist.html#refernces",
    "title": "Trojan Attack on MNIST",
    "section": "Refernces",
    "text": "Refernces\n\nTrojDiff: Trojan Attacks on Diffusion Models with Diverse Targets\nHow to Defend Neural Networks from Neural Trojan Attacks\n\nAuthor Details\n\nName: Akansh Maurya\nGithub: https://akansh12.github.io/\nLinkedin: Akansh Maurya\nEmail: akanshmaurya@gmail.com",
    "crumbs": [
      "Trojan Attack on MNIST"
    ]
  },
  {
    "objectID": "ddpm.html",
    "href": "ddpm.html",
    "title": "Denoising Diffusion Probabilistic Models(DDPM)",
    "section": "",
    "text": "Generated output on MNIST",
    "crumbs": [
      "Denoising Diffusion Probabilistic Models(DDPM)"
    ]
  },
  {
    "objectID": "ddpm.html#what-is-diffusion",
    "href": "ddpm.html#what-is-diffusion",
    "title": "Denoising Diffusion Probabilistic Models(DDPM)",
    "section": "What is diffusion?",
    "text": "What is diffusion?\n\n\n\nInk drop in a water\n\n\nDiffusion process : A diffusion process is stochastic markov process having continuous sample path. A process of moving from Complex Distribution to Simple Distribution. It has following properties:\n\nStochastic\nMarkov Chain\nContinuous sample path.",
    "crumbs": [
      "Denoising Diffusion Probabilistic Models(DDPM)"
    ]
  },
  {
    "objectID": "ddpm.html#what-is-ddpm",
    "href": "ddpm.html#what-is-ddpm",
    "title": "Denoising Diffusion Probabilistic Models(DDPM)",
    "section": "What is DDPM?",
    "text": "What is DDPM?\nDenoising Diffusion Probabilistic Models, DDPM in short, a paper by Jonathan Ho et al, defines a new class of generative models. Diffusion Models belong to the category of generative models, which are utilized to produce data resembling the training dataset. In essence, Diffusion Models operate by perturbing training data with incremental Gaussian noise and subsequently learning to reconstruct the original data by reversing this noise-induced degradation. Post-training, the Diffusion Model can be employed to generate data by feeding randomly sampled noise through the acquired denoising mechanism.\nWe can devide DDPM into two main components:\n\nForward/diffusion Process\nReverse/Sampling Process",
    "crumbs": [
      "Denoising Diffusion Probabilistic Models(DDPM)"
    ]
  },
  {
    "objectID": "ddpm.html#what-is-forwarddiffusion-process",
    "href": "ddpm.html#what-is-forwarddiffusion-process",
    "title": "Denoising Diffusion Probabilistic Models(DDPM)",
    "section": "What is forward/diffusion process?",
    "text": "What is forward/diffusion process?\nAs stated previously, a Diffusion Model involves a forward process, also known as a diffusion process, where a data point, typically an image, undergoes incremental noise addition. We perform this by using a Linear Noise Scheduler.\nConsidering a data point sampled from a genuine data distribution as \\(\\mathbf{x}_0 \\sim q(\\mathbf{x})\\), we introduce the concept of a “forward diffusion process.” In this process, Gaussian noise is incrementally added to the initial sample over \\(T\\) steps, resulting in a series of noisy samples denoted as \\(\\mathbf{x}_1, \\dots, \\mathbf{x}T\\). The magnitude of each step is determined by a variance schedule denoted as \\({\\beta_t \\in (0, 1)}{t=1}^T\\) \\[\nq(\\mathbf{x}_t \\vert \\mathbf{x}_{t-1}) = \\mathcal{N}(\\mathbf{x}_t; \\sqrt{1 - \\beta_t} \\mathbf{x}_{t-1}, \\beta_t\\mathbf{I}) \\quad\nq(\\mathbf{x}_{1:T} \\vert \\mathbf{x}_0) = \\prod^T_{t=1} q(\\mathbf{x}_t \\vert \\mathbf{x}_{t-1})\n\\]\nA nice property of the above diffusion process is that we can sample \\(\\mathbf{x}_t\\) from \\(\\mathbf{x}_0\\) using the equation:\n\\[\\begin{aligned}\nq(\\mathbf{x}_t \\vert \\mathbf{x}_0) &= \\mathcal{N}(\\mathbf{x}_t; \\sqrt{\\bar{\\alpha}_t} \\mathbf{x}_0, (1 - \\bar{\\alpha}_t)\\mathbf{I})\n\\end{aligned}\\]\nwhere: \\(\\alpha_t = 1 - \\beta_t\\) and \\(\\bar{\\alpha}_t = \\prod_{i=1}^t \\alpha_i\\)\n\n\n\nOpen In Colab\n\n\n\nimport torch\nimport os\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nimport numpy as np\nimport torchvision.transforms as transforms\nimport sys\nsys.path.append('../')\nfrom tqdm.auto import tqdm\nfrom torchvision.utils import make_grid\nfrom models import Unet\nfrom tqdm.auto import tqdm\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")",
    "crumbs": [
      "Denoising Diffusion Probabilistic Models(DDPM)"
    ]
  },
  {
    "objectID": "ddpm.html#noise-scheduler",
    "href": "ddpm.html#noise-scheduler",
    "title": "Denoising Diffusion Probabilistic Models(DDPM)",
    "section": "Noise Scheduler",
    "text": "Noise Scheduler\nWe will start by implementing the basic building block of DDPM with Noise Scheduler. It takes in num of timesteps, beta_start and beta_end as input. It returns a noised image at timestep t. Our Noise Scheduler class will have three components:\n\ninit(): This will pre-compute and store all the coefficient related to \\(\\alpha_{t}\\) and others.\nadd_noise(): This corresponds to forward process.\nsample_prev_timestep(): This is for reverse process and we will discuss it in later stage of this notebook.\n\n\nclass LinearNoiseScheduler():\n    def __init__(self, num_timesteps, beta_start, beta_end):\n        pass\n    def add_noise(self, original, noise, t):\n        pass\n    def sample_prev_timestep(self, xt, t, noise_pred):\n        pass\n\nAdd Noise and Init function\n\nclass LinearNoiseScheduler():\n    '''Inspired from: https://github.com/explainingai-code/DDPM-Pytorch'''\n    def __init__(self, num_timesteps, beta_start, beta_end):\n        self.num_timesteps = num_timesteps\n        self.beta_start = beta_start\n        self.beta_end = beta_end\n        self.betas = torch.linspace(beta_start, beta_end, num_timesteps)\n        self.betas = self.betas.to(device)\n        self.alphas = 1 - self.betas\n        self.alphas_cum_prod = torch.cumprod(self.alphas, 0)\n        self.sqrt_alphas_cum_prod = torch.sqrt(self.alphas_cum_prod)\n        self.sqrt_one_minus_alpha_cum_prod = torch.sqrt(1 - self.alphas_cum_prod)\n\n    def add_noise(self, original, noise, t):\n        original_shape = original.shape\n        batch_size = original_shape[0]\n\n        sqrt_alpha_cum_prod = self.sqrt_alphas_cum_prod[t].reshape(batch_size)\n        sqrt_one_minus_alpha_cum_prod = self.sqrt_one_minus_alpha_cum_prod[t].reshape(batch_size)\n\n        for _ in range(len(original_shape) - 1):\n            sqrt_alpha_cum_prod = sqrt_alpha_cum_prod.unsqueeze(-1)\n            sqrt_one_minus_alpha_cum_prod = sqrt_one_minus_alpha_cum_prod.unsqueeze(-1)\n\n        return sqrt_alpha_cum_prod * original + sqrt_one_minus_alpha_cum_prod * noise.to(original.device)\n\n    def sample_prev_timestep(self, xt, t, noise_pred):\n        pass\n\nWe can look at the values of alphas, betas and other for better understanding. As we have implemented linear scheduler, the value of \\(\\alpha\\) decrease with time which when looked in perspective of forward equation as mentioned earlier means that original image is decaying. While increasing value of \\(\\beta\\) show increase in gaussian noise component.\n\nlinear_scheduler = LinearNoiseScheduler(1000, 0.001, 0.02)\n\nplt.figure(figsize=(15,3))\nplt.subplot(1,4,1)\nplt.plot(linear_scheduler.alphas.cpu())\nplt.xlabel('Timestep')\nplt.ylabel('Alpha')\nplt.title('Alphas')\n\nplt.subplot(1,4,2)\nplt.plot(linear_scheduler.betas.cpu())\nplt.xlabel('Timestep')\nplt.ylabel('Beta')\nplt.title('Betas')\n\nplt.subplot(1,4,3)\nplt.plot(linear_scheduler.sqrt_alphas_cum_prod.cpu())\nplt.xlabel('Timestep')\nplt.ylabel('Sqrt Alpha Cum Prod')\nplt.title('Sqrt Alpha Cum Prod')\n\nplt.subplot(1,4,4)\nplt.plot(linear_scheduler.sqrt_one_minus_alpha_cum_prod.cpu())\nplt.xlabel('Timestep')\nplt.ylabel('Sqrt One Minus Alpha Cum Prod')\nplt.title('Sqrt One Minus Alpha Cum Prod')\n\nText(0.5, 1.0, 'Sqrt One Minus Alpha Cum Prod')\n\n\n\n\n\n\n\n\n\nDeffusion process on 2D image.\n\ntest_img = Image.open(\"./images/cameraman.jpg\")\ntest_img = test_img.resize((128, 128))\ntest_img = transforms.ToTensor()(test_img).unsqueeze(0)\ntest_img = test_img.to(device)\nstep = [0, 10, 50, 100, 200, 400, 500, 600,999]\nplt.figure(figsize=(25,15))\nplt.subplot(1,10,1)\nplt.imshow(np.transpose(test_img[0].cpu().numpy(), (1,2,0)))\nplt.title('Original')\nplt.axis('off');\nfor i, j in enumerate(step):\n    plt.subplot(1,10,i+2)\n    noise = torch.randn_like(test_img)\n    test_img_noisy = linear_scheduler.add_noise(test_img, noise, j)\n    plt.imshow(np.transpose(torch.clamp(test_img_noisy[0], 0, 1).cpu().numpy(), (1,2,0)))\n    plt.axis('off');\n    plt.title(f'Timestep {j}')",
    "crumbs": [
      "Denoising Diffusion Probabilistic Models(DDPM)"
    ]
  },
  {
    "objectID": "ddpm.html#what-is-reverse-diffusion-process",
    "href": "ddpm.html#what-is-reverse-diffusion-process",
    "title": "Denoising Diffusion Probabilistic Models(DDPM)",
    "section": "What is Reverse diffusion process?",
    "text": "What is Reverse diffusion process?\nThe magic of DDPM lies in the reverse process. In reverse process, we transform noise back into a sample from the target distribution.\nIf we are able to invert the aforementioned process and sample from \\(q(\\mathbf{x}_{t-1} \\vert \\mathbf{x}_t)\\), we can reconstruct the original sample from a Gaussian noise input, denoted as \\(\\mathbf{x}_T \\sim \\mathcal{N}(\\mathbf{0}, \\mathbf{I})\\). It’s important to note that when \\(\\beta_t\\) is sufficiently small, \\(q(\\mathbf{x}_{t-1} \\vert \\mathbf{x}_t)\\) also approximates a Gaussian distribution. However, estimating \\(q(\\mathbf{x}_{t-1} \\vert \\mathbf{x}_t)\\) directly is challenging since it requires leveraging the entire dataset. Therefore, to perform the reverse diffusion process, we need to train a model \\(p_\\theta\\) to approximate these conditional probabilities.\nThe equations governing this process are as follows:\n\\[\\begin{align*}\np_\\theta(\\mathbf{x}_{0:T}) &= p(\\mathbf{x}_T) \\prod^T_{t=1} p_\\theta(\\mathbf{x}_{t-1} \\vert \\mathbf{x}_t) \\\\\np_\\theta(\\mathbf{x}_{t-1} \\vert \\mathbf{x}_t) &= \\mathcal{N}(\\mathbf{x}_{t-1}; \\boldsymbol{\\mu}_\\theta(\\mathbf{x}_t, t), \\boldsymbol{\\Sigma}_\\theta(\\mathbf{x}_t, t))\n\\end{align*}\\]\nFor better understanding the equations, I highly recommend reading the blog by Lilian Weng: What are Diffusion Models? The forward and reverse process eqautions can be summarized by the following image.\n\n\n\nForward and Sampling equations\n\n\nNow we know the reverse sampling process, we can modify our LinearNoiseScheduler to accomodate the reverse process.\n\nclass LinearNoiseScheduler():\n    '''Inspired from: https://github.com/explainingai-code/DDPM-Pytorch'''\n    def __init__(self, num_timesteps, beta_start, beta_end):\n        self.num_timesteps = num_timesteps\n        self.beta_start = beta_start\n        self.beta_end = beta_end\n        self.betas = torch.linspace(beta_start, beta_end, num_timesteps)\n        self.betas = self.betas.to(device)\n        self.alphas = 1 - self.betas\n        self.alphas_cum_prod = torch.cumprod(self.alphas, 0)\n        self.sqrt_alphas_cum_prod = torch.sqrt(self.alphas_cum_prod)\n        self.sqrt_one_minus_alpha_cum_prod = torch.sqrt(1 - self.alphas_cum_prod)\n\n    def add_noise(self, original, noise, t):\n        original_shape = original.shape\n        batch_size = original_shape[0]\n\n        sqrt_alpha_cum_prod = self.sqrt_alphas_cum_prod[t].reshape(batch_size)\n        sqrt_one_minus_alpha_cum_prod = self.sqrt_one_minus_alpha_cum_prod[t].reshape(batch_size)\n\n        for _ in range(len(original_shape) - 1):\n            sqrt_alpha_cum_prod = sqrt_alpha_cum_prod.unsqueeze(-1)\n            sqrt_one_minus_alpha_cum_prod = sqrt_one_minus_alpha_cum_prod.unsqueeze(-1)\n\n        return sqrt_alpha_cum_prod * original + sqrt_one_minus_alpha_cum_prod * noise.to(original.device)\n\n    def sample_prev_timestep(self, xt, t, noise_pred):\n        x0 = (xt - self.sqrt_one_minus_alpha_cum_prod[t] * noise_pred)/(self.sqrt_alphas_cum_prod[t])\n        x0 = torch.clamp(x0, -1, 1)\n\n        mean = xt - ((self.betas[t])*noise_pred)/(self.sqrt_one_minus_alpha_cum_prod[t])\n        mean = mean/torch.sqrt(self.alphas[t])\n\n        if t == 0:\n\n            return mean, mean\n        else:\n            variance = (1 - self.alphas_cum_prod[t-1])/(1 - self.alphas_cum_prod[t])\n            variance = variance*self.betas[t]\n            sigma = torch.sqrt(variance)\n\n            z = torch.randn_like(xt).to(xt.device)\n\n            return mean + sigma*z, x0",
    "crumbs": [
      "Denoising Diffusion Probabilistic Models(DDPM)"
    ]
  },
  {
    "objectID": "ddpm.html#ddpm-training",
    "href": "ddpm.html#ddpm-training",
    "title": "Denoising Diffusion Probabilistic Models(DDPM)",
    "section": "DDPM Training",
    "text": "DDPM Training\n\n\nThe training and sampling algorithms in DDPM (Image source: Ho et al. 2020)",
    "crumbs": [
      "Denoising Diffusion Probabilistic Models(DDPM)"
    ]
  },
  {
    "objectID": "ddpm.html#data-preparation-dataset-and-dataloder",
    "href": "ddpm.html#data-preparation-dataset-and-dataloder",
    "title": "Denoising Diffusion Probabilistic Models(DDPM)",
    "section": "Data preparation, Dataset and Dataloder",
    "text": "Data preparation, Dataset and Dataloder\nFor setting up the dataset: * Download the csv files for Mnist and save them under data/MNIST_datadirectory.\nVerify the data directory has the following structure:\ndata/MNIST_data/train/images/{0/1/.../9}\n    *.png\ndata/MNIST_data/test/images/{0/1/.../9}\n    *.png\nYou can also run the following hidden cell(in Google Colab or local) to create the dataset as specified.\n\nfrom dataset import Image_Dataset\nfrom torch.utils.data import DataLoader\nmnist_data = Image_Dataset(\"../data/MNIST_data/train/images/\", transform=None, im_ext = '*.png')\nmnist_dataloader = DataLoader(mnist_data, batch_size=64, shuffle=True, num_workers=4)\n\nVerifying the size of input data.\n\nfor x,y in mnist_data:\n    print(x.shape)\n    print(y)\n    break\n\ntorch.Size([1, 28, 28])\ntensor(9)",
    "crumbs": [
      "Denoising Diffusion Probabilistic Models(DDPM)"
    ]
  },
  {
    "objectID": "ddpm.html#unet-model",
    "href": "ddpm.html#unet-model",
    "title": "Denoising Diffusion Probabilistic Models(DDPM)",
    "section": "Unet Model",
    "text": "Unet Model\nFor generation of image, we need a model architecture that has encoder-decoder components. Here we have used UNet with attention layers for image generation process.\nThe code of Unet is inspired from here.\n\nimport yaml\nconfig_path = \"../config/default.yaml\"\nwith open(config_path, 'r') as file:\n    try:\n        config = yaml.safe_load(file)\n    except yaml.YAMLError as exc:\n        print(exc)\n\n\nmodel = Unet(config['model_params'])\nmodel.to(device)\nnum_epochs = 40\noptimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\ncriterion = torch.nn.MSELoss()\nscheduler = LinearNoiseScheduler(1000, 0.0001, 0.02)\nnum_timesteps = 1000",
    "crumbs": [
      "Denoising Diffusion Probabilistic Models(DDPM)"
    ]
  },
  {
    "objectID": "ddpm.html#training-loop",
    "href": "ddpm.html#training-loop",
    "title": "Denoising Diffusion Probabilistic Models(DDPM)",
    "section": "Training Loop",
    "text": "Training Loop\n\n# Training loop\nfor epoch_idx in range(num_epochs):\n    epoch_losses = []\n    # Iterate through the data loader\n    for images, _ in tqdm(mnist_dataloader):\n        optimizer.zero_grad()\n        images = images.float().to(device)\n\n        # Generate random noise\n        noise = torch.randn_like(images).to(device)\n\n        # Randomly select time step\n        timestep = torch.randint(0, num_timesteps, (images.shape[0],)).to(device)\n\n        # Introduce noise to images based on time step\n        noisy_images = scheduler.add_noise(images, noise, timestep)\n\n        # Forward pass\n        noise_prediction = model(noisy_images, timestep)\n\n        # Calculate loss\n        loss = criterion(noise_prediction, noise)\n        epoch_losses.append(loss.item())\n\n        # Backpropagation\n        loss.backward()\n        optimizer.step()\n\n    # Print epoch information\n    print('Epoch:{} | Mean Loss: {:.4f}'.format(\n        epoch_idx + 1,\n        np.mean(epoch_losses),\n    ))\n\n    # Save model weights\n    torch.save(model.state_dict(), \"../model_weights/ddpm_ckpt.pth\")\n\nprint('Training Completed!')",
    "crumbs": [
      "Denoising Diffusion Probabilistic Models(DDPM)"
    ]
  },
  {
    "objectID": "ddpm.html#inference-and-sampling",
    "href": "ddpm.html#inference-and-sampling",
    "title": "Denoising Diffusion Probabilistic Models(DDPM)",
    "section": "Inference and Sampling",
    "text": "Inference and Sampling\ndownloading the trained weights, please use the this link and save them under /model_weights/directory.\n\nmodel.load_state_dict(torch.load(f'../model_weights/ddpm_ckpt.pth'))\nmodel.eval();\n\n\ndef sampling_grid(model, scheduler, num_timesteps, num_samples = 1, img_dim = 28, img_channels = 1):\n    model.to(device)\n    model.eval()\n    xt = torch.randn(num_samples, img_channels, img_dim, img_dim).to(device).to(device)\n    images = []\n    for t in tqdm(reversed(range(num_timesteps))):\n        t = torch.as_tensor(t).unsqueeze(0).to(device)\n        noise_pred = model(xt, t)\n        xt, x0 = scheduler.sample_prev_timestep(xt, t, noise_pred)\n        ims = torch.clamp(xt, -1., 1.).detach().cpu()\n        ims = (ims + 1) / 2\n        grid_img = make_grid(ims, nrow=10)\n        out_ing = transforms.ToPILImage()(grid_img)\n        os.makedirs(\"./images/sampling_out/ddpm_sample/\", exist_ok = True)\n        out_ing.save(f'./images/sampling_out/ddpm_sample/timestep_{t.cpu().numpy()}.png')\n        out_ing.close()\n\ndef sampling(model, scheduler, num_timesteps, num_samples = 1, img_dim = 28, img_channels = 1):\n    model.to(device)\n    model.eval()\n    xt = torch.randn(num_samples, img_channels, img_dim, img_dim).to(device).to(device)\n    images = []\n    for t in tqdm(reversed(range(num_timesteps))):\n        t = torch.as_tensor(t).unsqueeze(0).to(device)\n        noise_pred = model(xt, t)\n        xt, x0 = scheduler.sample_prev_timestep(xt, t, noise_pred)\n        ims = torch.clamp(xt, -1., 1.).detach().cpu()\n        ims = (ims + 1)/2\n        img = transforms.ToPILImage()(ims.squeeze(0))\n        images.append(img)\n    return images\n\n\nscheduler = LinearNoiseScheduler(1000, 0.0001, 0.02)\nwith torch.no_grad():\n    images = sampling_grid(model, scheduler, 1000, 100, 28, 1)\n\nwith torch.no_grad():\n    img = sampling(model, scheduler, 1000, 1)",
    "crumbs": [
      "Denoising Diffusion Probabilistic Models(DDPM)"
    ]
  },
  {
    "objectID": "ddpm.html#result",
    "href": "ddpm.html#result",
    "title": "Denoising Diffusion Probabilistic Models(DDPM)",
    "section": "Result",
    "text": "Result\n\nfrom PIL import Image\nimport matplotlib.pyplot as plt\n\nselected_images = img[::99]\n\n# Plot only 8 images from the selected_images list\nnum_images_to_plot = 11\nfig, axes = plt.subplots(1, num_images_to_plot, figsize=(20, 5))\n\n# Plot each selected image\nfor i, img_ in enumerate(selected_images[:num_images_to_plot]):\n    axes[i].imshow(img_, cmap = 'gray')\n    axes[i].axis('off')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nimport os\nimport imageio\nimage_dir = './images/sampling_out/ddpm_sample/'\n\nimage_files = sorted([os.path.join(image_dir, file) for file in os.listdir(image_dir) if file.endswith('.png')], reverse=True)\n\nselected_images = []\nfor i, image_file in enumerate(image_files):\n    if i % 25 == 0:\n        selected_images.append(os.path.join(image_dir, f\"timestep_[{i}].png\"))\n\ngif_images = []\nfor i in range(len(selected_images)-1, 0, -1):\n    gif_images.append(imageio.imread(selected_images[i]))\n\noutput_gif_path = './images/output_benign.gif'\n\nimageio.mimsave(output_gif_path, gif_images, duration=100)\n\n\nfrom IPython.display import Image\n# Path to your GIF file\ngif_path = './images/output_benign.gif'\n# Display the GIF\nImage(filename=gif_path)\n\n&lt;IPython.core.display.Image object&gt;\n\n\n\n\n\nGenerated output on MNIST",
    "crumbs": [
      "Denoising Diffusion Probabilistic Models(DDPM)"
    ]
  },
  {
    "objectID": "ddpm.html#refernces",
    "href": "ddpm.html#refernces",
    "title": "Denoising Diffusion Probabilistic Models(DDPM)",
    "section": "Refernces",
    "text": "Refernces\n\nWhat are Diffusion Models? by Weng, Lilian\nIntroduction to Diffusion Models for Machine Learning\nThe way of writing the code is inspired from: https://github.com/explainingai-code\nDenoising Diffusion Probabilistic Models\n\nPlease check: Seminar presentation Link by me: Presentation\nAuthor Details\n\nName: Akansh Maurya\nGithub: https://akansh12.github.io/\nLinkedin: Akansh Maurya\nEmail: akanshmaurya@gmail.com",
    "crumbs": [
      "Denoising Diffusion Probabilistic Models(DDPM)"
    ]
  }
]